---
layout: post
title:  "웹 파이썬 크롤링"
date:   2019-02-04 16:43:52
author: devAhn
categories: Devlog
cover:  "/assets/data.jpeg"
---


파이썬 웹 크롤링을 학기중에 제대로 마무리를 못지은게 아쉬워 방중에 다시 시도하고 있다. 


마침 교환학생에 관심이 생겨 학교 국제교류처 홈페이지에 학과별 수기를 찾아보았다. 나랑 관련있는 전자공학과 및 공대 학우분들의 수기가 타학과에 비해서


매우 적었고, 학과 별 검색 필터링이 안되어서 불편했다. 이 문제를 크롤링을 통해 직접 정렬하여 해결해보기로 했다.






***문제상황: 너무 많은 수기, 필터 검색 기능이 없음.***

**원하는 해결: 그 중에 공학계열, 전자공학과 학우분의 수기만 골라서 보고싶음**


{% highlight python %}
!pip install requests
import pandas as pd
from bs4 import BeautifulSoup as bs
import requests
{% endhighlight %}


### 시작 전 필요한 셋업들이다.(Anaconda 의 Jupyternotebook 기반으로 작성하였다.)


아래 라이브러리는 해놓으면 설치진도를 눈으로 확인할 수 있어 에러가 난 것인지 실행하는데 시간이 오래 걸리는 것인지 알 수 있다.


{% highlight python %}
!pip install tqdm
from tqdm import *
{% endhighlight %}


반복문에서 for i in tqdm(): 
괄호 밖에 tqdm을 써주면 된다. 아래와 같이 표시됌

<img src=“devahn.github.io/assets/스크린샷 2019-02-04 오후 4.55.06.png" title=“tqdm”>



설명에 앞서 코드 전문을 게시한 후 한 줄씩 설명하겠다.

{% highlight python %}
a=[]
for i in tqdm(range(1,100)):
    num=str(i)
    url = ‘http://oiak.khu.ac.kr/program/memoirs.php?‘+‘page=’+num+‘&perpage=15&b_ex1=&s1=&s2=&b_code=12’
    
    r= requests.get(url)
    r.text
    soup=bs(r.text,‘lxml’)
    a.extend(soup.findAll(‘td’)[2::5])
 
for i in range(len(a)):
    a[i]=a[i].text
 
    
    
url2 = (‘http://oiak.khu.ac.kr/program/memoirs.php?‘+‘page=’+‘1’+‘&perpage=15&b_ex1=&s1=&s2=&b_code=12’)
 
r= requests.get(url2)
 
r.text
 
soup2=bs(r.text,‘lxml’)
 
first=soup2.findAll(‘td’)[0].text
 
first=int(first)
for i in range(len(a)):
    if ‘전자’ in a[i]:
        print(a[i],first-i)
{% endhighlight %}

a=[] # 빈 리스트를 만들어준다. 나중에 리스트에 내가 원하는 값을 넣어주기 위해서.

url = 'http://oiak.khu.ac.kr/program/memoirs.php?'+'page='+num+'&perpage=15&b_ex1=&s1=&s2=&b_code=12'
    
r= requests.get(url) #url의 모든 정보를 모듈을 통해 불러오고,

r.text # 그 url의 모든 정보를 난잡한 형태로 불러온다. 

soup=bs(r.text,'lxml') # 태그별로 깔끔하게 정리해서 lxml, html 형식으로 soup에 넣어준다.

a.extend(soup.findAll('td')[2::5]) # soup.findAll('td')는 td태그를 모두 불러와 리스트 형식으로 저장한다.


  ''' 두번째꺼부터 5칸 간격으로 소속대학 및 학과가 나오길래 [2::5] 로 처리하였다.'''
  
 수기는 교환학생 과정을 수료한 학생들이 종료마다 작성하므로 매일 업데이트 된다. 내가 처음 짠 코드는 내가 직접 첫번쨰 수기 값의 번호를 입력해줘야했다. 만들고 나니 욕심이 생겨 첫 번쨰 수기의 순서를 자동으로 받아오게 하고 싶어 뒤 부분을 추가 수정 하였다.
 
url2 = 'http://oiak.khu.ac.kr/program/memoirs.php?'+'page='+'1'+'&perpage=15&b_ex1=&s1=&s2=&b_code=12'
  
 .# 사이트가 매일 업데이트 되므로
  
r= requests.get(url2)

 .# 위와 같이 request.get 으로 아까와 동일하게 페이지 정보를 갖고오고.
 
r.text
soup=bs(r.text,'lxml')
first=soup.findAll('td')[0].text #여기서 첫번쨰 수기의 번호를 자동으로 가져오게 했다.
first=int(first)


for i in range(len(a)):
    if '전자' in a[i]:
        print(a[i],first-i) # '전자' 가 포함된 항목만 걸러내도록 하였다.
         
다음이 결과 값이다. 

![스크린샷 2019-02-04 오후 8.47.22.png](/Users/bigfootboy24-mac/Desktop/스크린샷 2019-02-04 오후 8.47.22.png)


        
 ###### 이런 식으로 결과값으로 내가 원하는 값인 '전자공학과와 해당 글의 번호' 가 함께 출력되도록 하였다.
 
 여기까지가 첫번쨰 마무리였다. 만들고 보니 내가 직접 1366까지 페이지를 넘기며 가야해서 더 편리한, 쉬운 방법을 찾으려고 해봤다. 
 **개별 링크를 보여주면 더 편하겟네!**
라는 생각이 들었고 바로 실행에 옮겼다.  

{% highlight python %}
b=[]
for i in tqdm(range(1,20)):
    num=str(i)
    url = ‘http://oiak.khu.ac.kr/program/memoirs.php?‘+‘page=’+num+‘&perpage=15&b_ex1=&s1=&s2=&b_code=12’
    
    r= requests.get(url)
    r.text
    soup=bs(r.text,‘lxml’)
    b.extend(soup.findAll(‘td’,{‘class’:‘title’}))
                 
 
                          #href=True))
                 
                          
             #{‘class’:’title’}))
    #반복문은 수행만 할뿐 저장하지 않는다. 처음에 어떤 에러가 생겼었는지 잘생각해보기 반복만 하고 저장은 안하니까
#그냥 마지막 값을 출력한거야.
{% endhighlight %}

 
 
 처음에 링크를 하나만 뽑아주는 내 의도와 다른 결과가 계속 나왔었다. 여기서 내가 잘못 생각한 것은 *반복문은 시행만 여러번 할뿐 저장은 하지 않는 다*는 점이었다. 이 부분을 다음에 수정할 계획이다. 다음 포스팅에서 계속 하겠다. 
 
 공부와 떨어진 ***정말 내가 필요해서 짠*** 첫번째 코드 결과물이라 뿌듯했다. 
 
 가고싶은 분야는 자동차 및 로봇이지만 WEB과 Data 다루는 능력은 가끔 하기 재밌을 거 같아 틈틈이 하려고 한다. :)
